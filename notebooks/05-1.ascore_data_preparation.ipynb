{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T13:40:26.039459Z",
     "start_time": "2025-03-26T13:40:25.222838Z"
    }
   },
   "source": [
    "# read../visualization/assets/data/matrix/new_community_evolution.csv\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "handle_df = pd.read_csv(\"../visualization/assets/data/matrix/new_community_evolution.csv\")\n",
    "# ",
    "# handle, 320\n",
    "import numpy as np\n",
    "\n",
    "RAD = np.pi / 180\n",
    "\n",
    "# addprocessNaN\n",
    "def handle_nan(obj):\n",
    " \"\"\"processJSONcolumnNaN, null\"\"\"\n",
    " if isinstance(obj, dict):\n",
    " return {k: handle_nan(v) for k, v in obj.items()}\n",
    " elif isinstance(obj, list):\n",
    " return [handle_nan(i) for i in obj]\n",
    " elif isinstance(obj, float) and np.isnan(obj):\n",
    " return None\n",
    " else:\n",
    " return obj\n",
    "\n",
    "def polarToCartesian(asrank, attr, radius=350):\n",
    " \"\"\"\n",
    " xy\n",
    " :param asrank: processasrankdata, DataFrameformat, longitude, attrfield\n",
    " :param attr: as, cone\n",
    " :param radius: node\n",
    " :return:\n",
    " \"\"\"\n",
    " # mappingconemin-max mapping 0-350\n",
    " min_cone = asrank[attr].min()\n",
    " max_cone = asrank[attr].max()\n",
    "\n",
    " # cone, , ",
    " max_Domain = 1 - np.log((min_cone + 1) / (max_cone + 1))\n",
    "\n",
    " # conelogprocess\n",
    " asrank['cone_map'] = asrank[attr].apply(lambda x: (1 - np.log((x + 1) / (max_cone + 1))))\n",
    "\n",
    " # logprocess, cone0-350mapping\n",
    " asrank['r'] = asrank['cone_map'].apply(lambda x: (((x - 1) / (max_Domain - 1)) * radius))\n",
    "\n",
    " # xy x=+rcosÎ¸\n",
    " x = 1900 / 2 + asrank['r'] * np.cos(asrank['longitude'] * RAD)\n",
    " y = 1000 / 2 - asrank['r'] * np.sin(asrank['longitude'] * RAD)\n",
    "\n",
    " asrank['x'] = x\n",
    " asrank['y'] = y\n",
    " # rank rfield\n",
    " asrank.sort_values(by=attr, ascending=False, inplace=True)\n",
    " asrank.drop_duplicates(subset=['id', attr], keep='last', inplace=True)\n",
    "\n",
    " return asrank\n",
    "\n",
    "\n",
    "# for date in range(202404, 202411):\n",
    "# cord_rank = pd.read_csv(f'./{date}/handle/rank{date}.csv')\n",
    "# cord_rank1 = pd.read_csv(f'./{date}/rank{date}.csv')\n",
    "# cord_rank = polarToCartesian(cord_rank, 'cone', 320)\n",
    "# cord_rank1 = polarToCartesian(cord_rank1, 'cone', 320)\n",
    "# cord_rank.fillna(0, inplace=True)\n",
    "# cord_rank1.fillna(0, inplace=True)\n",
    "# cord_rank.to_csv(f'./{date}/handle/rank{date}.csv', index=False)\n",
    "# cord_rank1.to_csv(f'./{date}/rank{date}.csv', index=False)\n",
    "\n",
    "\n",
    "base_dir = './'\n",
    "\n",
    "# Initialize rankjson and reljson\n",
    "rankjson = {}\n",
    "reljson = {}\n",
    "\n",
    "# Get all unique values of each column in handle_df\n",
    "for col in handle_df.columns:\n",
    " unique_values = handle_df[col].unique() # columnunique values\n",
    " rankjson[col] = {} # createcolumn rankjson ",
    " reljson[col] = {} # createcolumn reljson ",
    "\n",
    " # columnunique values\n",
    " for value in unique_values:\n",
    " # value , JSON ",
    " value_str = str(value)\n",
    "\n",
    " # Get rank and rel file paths by column name\n",
    " rank_path = os.path.join(base_dir, f'{col}/handle/rank{col}.csv')\n",
    " rel_path = os.path.join(base_dir, f'{col}/handle/rel{col}.csv')\n",
    "\n",
    " # read rank rel file\n",
    " temp_rank = pd.read_csv(rank_path)\n",
    " temp_rel = pd.read_csv(rel_path)\n",
    "\n",
    " # Filter data by conditions\n",
    " filtered_rank = temp_rank[temp_rank['community'] == value]\n",
    " filtered_rel = temp_rel[temp_rel['type'] == str(value)]\n",
    " filtered_rank = polarToCartesian(filtered_rank, 'cone', 320)\n",
    "\n",
    " if len(filtered_rank) != 0 and len(filtered_rel) != 0:\n",
    " filtered_rel['cone'] = filtered_rel.apply(\n",
    " lambda row: max(\n",
    " filtered_rank.loc[filtered_rank['id'] == row['source'], 'cone'].iloc[0], # source cone ",
    " filtered_rank.loc[filtered_rank['id'] == row['target'], 'cone'].iloc[0] # target cone ",
    " ),\n",
    " axis=1\n",
    " )\n",
    "\n",
    " os.makedirs(f'./local_core/{col}/raw', exist_ok=True)\n",
    " # filter rankjson reljson ",
    " # NaNNone, JSONcolumn\n",
    " rankjson[col][value_str] = handle_nan(filtered_rank.replace({np.nan: None}).to_dict(orient='records'))\n",
    " reljson[col][value_str] = handle_nan(filtered_rel.replace({np.nan: None}).to_dict(orient='records'))\n",
    "\n",
    " with open(f'./local_core/{col}/raw/rank.json', 'w', encoding='utf-8') as rank_file:\n",
    " json.dump(rankjson, rank_file, ensure_ascii=False, indent=4)\n",
    "\n",
    " with open(f'./local_core/{col}/raw/rel.json', 'w', encoding='utf-8') as rel_file:\n",
    " json.dump(reljson, rel_file, ensure_ascii=False, indent=4)\n",
    " print(f\"Saved data for {col}\")\n",
    "\n",
    "\n",
    "# node, , relation, export\n",
    "def process_rank_and_rel(t1, t2):\n",
    " t1.fillna(0, inplace=True)\n",
    " t1_agg = t1.loc[t1.groupby('country')['cone'].idxmax()]\n",
    "\n",
    " t1_agg['cone'] = t1_agg['country'].apply(\n",
    " lambda country: t1[t1['country'] == country]['cone'].sum()\n",
    " )\n",
    " t1_agg = polarToCartesian(t1_agg, 'cone', 320)\n",
    "\n",
    " id_mapping = {}\n",
    " for idx, row in t1.iterrows():\n",
    " country = row['country']\n",
    " key = row['id']\n",
    " value = t1_agg[t1_agg['country'] == country]['id'].values[0]\n",
    " id_mapping[key] = value\n",
    "\n",
    " t2['source'] = t2['source'].map(id_mapping).fillna(t2['source'])\n",
    " t2['target'] = t2['target'].map(id_mapping).fillna(t2['target'])\n",
    " t2['source'] = t2['source'].astype(int)\n",
    " t2['target'] = t2['target'].astype(int)\n",
    " t2 = t2[t2['source'] != t2['target']]\n",
    " t2['source_target'] = t2.apply(lambda row: tuple(sorted([row['source'], row['target']])), axis=1)\n",
    " t2['reverse_relation'] = t2.apply(lambda row: tuple(sorted([row['target'], row['source']])), axis=1)\n",
    " t2 = t2.drop_duplicates(subset=['source_target'])\n",
    " t2 = t2.drop_duplicates(subset=['reverse_relation'])\n",
    " t2 = t2.drop_duplicates(subset=['source', 'target'])\n",
    " t2 = t2.drop(columns=['source_target'])\n",
    " t2 = t2.drop(columns=['reverse_relation'])\n",
    "\n",
    " if len(t1_agg) != 0 and len(t2) != 0:\n",
    " t2['cone'] = t2.apply(\n",
    " lambda row: max(\n",
    " t1_agg.loc[t1_agg['id'] == row['source'], 'cone'].iloc[0], # source cone ",
    " t1_agg.loc[t1_agg['id'] == row['target'], 'cone'].iloc[0] # target cone ",
    " ),\n",
    " axis=1\n",
    " )\n",
    "\n",
    " return t1_agg, t2\n",
    "\n",
    "\n",
    "base_dir = './'\n",
    "\n",
    "# Initialize rankjson and reljson\n",
    "rankjson = {}\n",
    "reljson = {}\n",
    "\n",
    "# Get all unique values of each column in handle_df\n",
    "for col in handle_df.columns:\n",
    " unique_values = handle_df[col].unique() # columnunique values\n",
    " rankjson[col] = {}\n",
    " reljson[col] = {}\n",
    " os.makedirs(f'./local_core/{col}/agg', exist_ok=True)\n",
    " # columnunique values\n",
    " for value in unique_values:\n",
    " # value , JSON ",
    " value_str = str(value)\n",
    "\n",
    " # Get rank and rel file paths by column name\n",
    " rank_path = os.path.join(base_dir, f'{col}/handle/rank{col}.csv')\n",
    " rel_path = os.path.join(base_dir, f'{col}/handle/rel{col}.csv')\n",
    "\n",
    " # read rank rel file\n",
    " temp_rank = pd.read_csv(rank_path)\n",
    " temp_rel = pd.read_csv(rel_path)\n",
    "\n",
    " # Filter data by conditions\n",
    " filtered_rank = temp_rank[temp_rank['community'] == value]\n",
    " filtered_rel = temp_rel[temp_rel['type'] == str(value)]\n",
    " t1, t2 = process_rank_and_rel(filtered_rank, filtered_rel)\n",
    "\n",
    " if len(t1) == 1:\n",
    " t1.loc[t1.index[0], 'x'] = 1900 / 2\n",
    " t1.loc[t1.index[0], 'y'] = 1000 / 2\n",
    " t1.loc[t1.index[0], 'r'] = 0\n",
    " ",
    " # filter rankjson reljson , processNaN\n",
    " rankjson[col][value_str] = handle_nan(t1.replace({np.nan: None}).to_dict(orient='records'))\n",
    " reljson[col][value_str] = handle_nan(t2.replace({np.nan: None}).to_dict(orient='records'))\n",
    "\n",
    " with open(f'./local_core/{col}/agg/rank.json', 'w', encoding='utf-8') as rank_file:\n",
    " json.dump(rankjson, rank_file, ensure_ascii=False, indent=4)\n",
    "\n",
    " with open(f'./local_core/{col}/agg/rel.json', 'w', encoding='utf-8') as rel_file:\n",
    " json.dump(reljson, rel_file, ensure_ascii=False, indent=4)\n",
    " print(f\"Saved data for {col}\")\n",
    "\n",
    "### Logic for fdeb\n",
    "from common.utils.As_rank_rel import fdeb_format\n",
    "\n",
    "base_dir = './'\n",
    "\n",
    "# Initialize rankjson and reljson\n",
    "rankjson = {}\n",
    "reljson = {}\n",
    "\n",
    "# Get all unique values of each column in handle_df\n",
    "for col in handle_df.columns:\n",
    " unique_values = handle_df[col].unique() # columnunique values\n",
    " rankjson[col] = {}\n",
    " reljson[col] = {}\n",
    " os.makedirs(f'./local_core/{col}/fdeb', exist_ok=True)\n",
    " # columnunique values\n",
    " for value in unique_values:\n",
    " # value , JSON ",
    " value_str = str(value)\n",
    "\n",
    " # Get rank and rel file paths by column name\n",
    " rank_path = os.path.join(base_dir, f'{col}/handle/rank{col}.csv')\n",
    " rel_path = os.path.join(base_dir, f'{col}/handle/rel{col}.csv')\n",
    "\n",
    " # read rank rel file\n",
    " temp_rank = pd.read_csv(rank_path)\n",
    " temp_rel = pd.read_csv(rel_path)\n",
    "\n",
    " # Filter data by conditions\n",
    " filtered_rank = temp_rank[temp_rank['community'] == value]\n",
    " filtered_rel = temp_rel[temp_rel['type'] == str(value)]\n",
    " t1, t2 = process_rank_and_rel(filtered_rank, filtered_rel)\n",
    "\n",
    " if len(t1) == 1:\n",
    " t1.loc[t1.index[0], 'x'] = 1900 / 2\n",
    " t1.loc[t1.index[0], 'y'] = 1000 / 2\n",
    " t1.loc[t1.index[0], 'r'] = 0\n",
    "\n",
    " r1, r2 = fdeb_format(t1, t2)\n",
    " # processfdebformatdataNaN\n",
    " rankjson[col][value_str] = handle_nan(r1)\n",
    " reljson[col][value_str] = handle_nan(r2)\n",
    " ",
    " with open(f'./local_core/{col}/fdeb/rank.json', 'w', encoding='utf-8') as rank_file:\n",
    " json.dump(rankjson, rank_file, ensure_ascii=False, indent=4)\n",
    "\n",
    " with open(f'./local_core/{col}/fdeb/rel.json', 'w', encoding='utf-8') as rel_file:\n",
    " json.dump(reljson, rel_file, ensure_ascii=False, indent=4)\n",
    " print(f\"Saved data for {col}\")\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './202401/handle/rank202401.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m\n\u001b[1;32m 91\u001b[0m rel_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/handle/rel\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m 93\u001b[0m \u001b[38;5;66;03m# read rank rel file\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m temp_rank \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m 95\u001b[0m temp_rel \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(rel_path)\n\u001b[1;32m 97\u001b[0m \u001b[38;5;66;03m# Filter data by conditions\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/rpki-community/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m 1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m 1014\u001b[0m dialect,\n\u001b[1;32m 1015\u001b[0m delimiter,\n\u001b[0;32m (...)\u001b[0m\n\u001b[1;32m 1022\u001b[0m dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m 1023\u001b[0m )\n\u001b[1;32m 1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/rpki-community/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m 617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m 619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m 622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m 623\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.virtualenvs/rpki-community/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m 1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m 1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/rpki-community/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m 1878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m 1879\u001b[0m mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m 1881\u001b[0m \u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 1882\u001b[0m \u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 1883\u001b[0m \u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 1884\u001b[0m \u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 1885\u001b[0m \u001b[43m \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 1886\u001b[0m \u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 1887\u001b[0m \u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 1888\u001b[0m \u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m 1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m 1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.virtualenvs/rpki-community/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m 868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m 869\u001b[0m \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m 870\u001b[0m \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m 871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m 872\u001b[0m \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m 874\u001b[0m \u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 875\u001b[0m \u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 876\u001b[0m \u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 877\u001b[0m \u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m 878\u001b[0m \u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m 879\u001b[0m \u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m 880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m 881\u001b[0m \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m 882\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './202401/handle/rank202401.csv'"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}